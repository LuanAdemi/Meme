

\section{Optimization Memory and Tag Width}

%In each motivating use case we have given, traffic equivalence classes are characterized by unique sets of attributes. We claim that we can construct a tag for each equivalence class which switches may use to determine the class's attributes using some set of wildcard rules. In this section, we will go over what those rules look like.

%In wildcard rule tables, a rule is a $<match, action>$ pair, where $match$ is a wildcard string which is compared to the packet header, and $action$ is the action the switch takes if the string comparison returns true. Match strings are strings over the alphabet $\{0,1,*\}$, where 0 and 1 are specific bit values, and $*$ denotes "don't care". Given a packet header consisting of bits $\{h_1,h_2,\ldots,h_n\}$ and a wildcard string $\{w_1,w_2,\ldots,w_n\}$, they are said to match if for every $i$, either $h_i = w_i$ or $w_i = *$. 

It is our goal to construct a tag for each FEC such that if an attribute exists across many sets, a single set of rules can determine if that attribute is true for any tag. Additionally, we wish for the maximum width of any tag to not be too large, and for the number of rules required by all attribute testing to be minimized. 

For each attribute $i$, a number of attribute tests $t_i$ will be performed. If $r_i$ rules are required to perform one test, then there are $t_i\cdot r_i$ rules associated with attribute $i$. The total number of rules required by a tag assignment is thus $\sum_i{t_i\cdot r_i}$. 


In the introduction, we discussed flat tags and attribute-carrying tags. We noted that flat tags optimally minimize tag width, and we gave a simple example of an attribute-carrying bitfield tag that minimized the memory used by switches. Each of the two examples minimized one desirable property while ignoring another. In the previous section, we devised a new format for attribute-carrying tags which achieves some middle-ground, giving better widths than bitfield tags, and better memory usage than flat tags. However, we did not discuss how to encode the given attribute sets into this format. 



%For this scheme of attaching information sets to packets to be feasible with commodity switches, it must be possible for membership testing to be implemented in the forwarding tables of switches. In switch TCAM tables, rules are are comparisons between a fixed string and the packet header, where the fixed strings are over the alphabet $\{0,1,*\}$. $0$ and $1$ are specific bit values, and $*$ denotes "don't care". We say that a packet header matches a string if for every bit in the header, either the bits are equal or one is a wildcard. Example usages include exact matches (strings with no wildcards), prefix matches (strings that end in wildcards), and reading of individual bits (strings with only one non-wildcard). 


 %(TODO: cite some tagging works like flowtags and the original SDX?) have solved the problem of associating packets with information sets by generating a tag for each unique information set and repurposing one of the fields in the header for the tag. In the FlowTags work, each middlebox path had its own set of tags, which could fit into the IP Fragment Identification field. To determine if middlebox $X$ is the next-hop, switches must compare the tag to every tag which has $X$ as a next-hop using TCAM exact matches. This can result in a TCAM entry count exponential in the number of bits in a tag. In the SDX work, each set of next-hops had a unique tag which was placed in the destination mac field. Again, to determine if next-hop $X$ is correct, the tag must be compared against every tag which contains $X$ using exact TCAM matches. 

%In both works, the number of bits required can be quite small, but membership tests are expensive, requiring rules exponential in the tag size. TCAM is a very limited resource, and it would be desirable to design tags with the goal of decreasing the number of entries required for membership testing. 

%To combat these issues, we present a compression scheme which allows
%the encoding of sequences over a large number of elements, such as service chains or lists of BGP next-hops, into a format easily queried by commodity switches. We show how, with an additional algorithm, this scheme can be used to compress both ordered and unordered sets. Finally, we evaluate our algorithms across both synthetic and real datasets, and show not only does the number of bits needed by our compression scheme compete with the number of bits needed by tags, but that that each core switch need only a constant number of entries per membership test, versus a linear number of entries for the case of flat tagging.




When encoding attribute sets into superset format, there are two different objectives to keep in mind: minimizing the number of bits required by the superset identifier and mask, and minimizing the cost of membership tests. If the goal was simply to minimize the number of bits required, we would have no bits for the mask and dedicate the entire tag to the identifier. This is flat tagging, and the number of TCAM entries required for membership tests would be maximized. If the number of bits for tagging was unconstrained, we would have no bits for superset identifier and simply have one large mask, causing each membership test to require only one TCAM entry. The problem only becomes interesting once both objectives are considered simultaneously, which we will now formalize. 

Let $\mathcal{A} = \{s_1, s_2, \dots, s_A \}$ be the list of distinct attribute sets for all seen equivalence classes, where each set $s_i$ is drawn from a universe of attributes $U = \{1, 2, \dots, N\}$. Associated with each element $j$ is a weight term $w_j$, which corresponds to the number of times attribute $j$ is read by network nodes. We are also given a hard limit on the width of equivalence class tags, denoted by $B$. 

Our goal is to construct a mapping $\mathnormal{f}$ such that 
$s_i \subseteq s_k \Leftrightarrow  \mathnormal{f}(i) = k \in \{1, \dots, Z\} $. In other words, we wish to choose a value $Z$, partition the input attribute sets into $Z$ groups, and union each group to create $Z$ supersets.

In these terms, the objective function we aim to minimize is

\begin{samepage}
$$ \min \sum_{j \in U} w_j \cdot |\{z_k | j \in z_k \}| $$

OR

$$ \min \sum_{j \in U} w_j \cdot \sum_{k \in [1\dots Z]}\mathbbm{1}_{j \in s_k} $$

Subject to

$$ \log_2{Z} + \max_{k \in [1\dots Z]}\{s_k\} \le B $$

\end{samepage}

The inner sum of the objective function denotes the number of supersets which attribute $j$ appears in. If this number is $x$, then there will be $x$ different types of tags which may contain $j$. To test for attribute $x$
This objective reflects the amount of memory which will be used by a tag set generated from the chosen supersets. 



%Where $a_j$ is the number of supersets that element $j \in U$ belongs to, and $B$ is the maximum number of bits that our tag can use. The objective function reflects the number of TCAM entries required for all membership tests. If the network tests for the membership of element $j$ $w_j$ times, and $j$ appears in $a_j$ supersets, then our approach requires $a_j w_j$ TCAM entries. 
The constraint shows that a superset construction is feasible if the identifier size plus the mask size is less than the bit constraint $B$. The identifier size is $\log_2{Z}$ if $Z$ is the number of supersets, and the mask size required is the maximum superset size, or $\max_{s_i \in S}\{s_i\}$.

%We suspect that this problem is NP-Complete for \mbox{$M > B$}, but a proof of the claim is left as future work. 

\subsection{A Greedy Algorithm}

Although the problem may be hard, we can still formulate heuristics for constructing good-enough solutions. We begin with $N$ supersets, where superset $i$ is the $i$th list wish wish to recover. This solution is, by construction, able to generate every required element list, but it may not be feasible. $N$ may be so large that the superset identifier will dominate the metadata and exceed the bit limit. This solution will also certainly require a large number of TCAM entries, as no sets have yet been combined into supersets.

In the first step of the algorithm, we delete any supersets which are subsets of other supersets. If superset $s_i$ is $[A,B,C]$, and superset $s_j$ is $[B,C]$, then there is no use for $s_j$ and it may be deleted. Deletion strictly improves the solution because any subset of $s_j$ can still be recovered from $s_i$, and the superset identifier decreases in size as the number of supersets decreases. In practice, this reduces the number of sets down from hundreds of thousands to less than one hundred.

In step two, we attempt to greedily decrease the number of bits required by our feasible solution by merging pairs of small supersets together that do not increase the maximum mask size when unioned. The idea is that this will decrease the number of sets and thus the size of the superset identifier will decrease. This step repeats until every feasible merging action would increase the number of bits required. 

In step three, we improve the remaining supersets in an iterative greedy fashion. We consider all feasible mergings of pairs of supersets, where a feasible merge is a union of the two supersets which does not result in the new mask size exceeding the bit limit. The \textit{benefit} of a merging is the decrease in the number of flow rules which will result
from the merge. The decrease is the sum of the weights of each participant which appears in the intersection of the two supersets, where the weight is the number of flow rules in which the participant appears. This is because after a merging of two sets, every participant which appeared in the intersection now appears one less time across the supersets, and thus every rule they appear in can be replicated one less time. 

With these definitions in mind, the full algorithm is as follows:

\begin{algorithm}
 \KwData{feasible supersets $S = \{s_1, s_2, \dots, s_M\}$}
 \KwResult{a list of supersets with maximally decreased rule inflation }
remove subsets from $S$\;
let $A =$ the set of merge pairs which don't increase bits required\;
\While{A is nonempty}{
  choose pair $(s_i, s_j) = a \in A$ with smallest union\;
  merge sets $s_i$ and $s_j$\;
  update $A$\;
 }
let $A =$ the set of feasible merge pairs\;
remove any subsets from
\While{A is nonempty}{
  choose $(s_i, s_j) = a \in A$ with greatest benefit\;
  merge sets $s_i$ and $s_j$\;
  update $A$\;
 }
\end{algorithm}

The loops consider all pairs of supersets in each iteration in the worst case, so they have a quadratic running time. Fortunately, the removal of subset supersets in the first step, which runs linearly on lists of supersets with high redundancy, reduces the number of supersets in such lists to a small constant, so the running time is reasonable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Handling Updates}

%We have given an algorithm for computing a static solution, but not all applications will be static. For middleboxes, new paths can be introduced, and existing paths can be modified. For IXP case, routes are announced and withdrawn continuously, meaning the list of valid next-hops for many prefixes are constantly changing. Therefore, we must give a procedure for handling dynamic updates to the superset matrix.

%We begin with a feasible solution as output by our algorithm, which is a collection of supersets and a mask for each element set. We consider both cases of a new set or a modified set identically. We first generate a new superset for the new/modified set. If this new superset is a subset of an existing superset, we remove it. 

%Each time a prefix is announced or withdrawn by a participant, we consider that prefix's new list of valid next-hops. If it is still a subset of some superset, we need only update the mask. 

%If the new set is not a subset of any superset, we attempt to merge the new superset with an existing superset via the same greedy procedure we applied to the static context. If no merging is possible, we leave it as its own superset. If the introduction of the new superset causes the bit limit to be exceeded, we recompute the static solution entirely as a worst-case scenario. In our evaluations, we have found the worst-case scenario is never needed.


