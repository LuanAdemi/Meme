

\section{The Formal Problem}
When constructing the supersets, there are two different objectives to keep in mind: minimizing the number of bits required by the superset identifier and mask, and minimizing the number of TCAM entries required for membership tests. If the goal was simply to minimize the number of bits required, we would have no bits for the mask and dedicate the entire tag to the identifier. This is flat tagging, and the number of TCAM entries required for membership tests would be maximized. If the number of bits for tagging was unconstrained, we would have no bits for superset identifier and simply have one large mask, causing each membership test to require only one TCAM entry. The problem only becomes interesting once both objectives are considered simultaneously, which we will now formalize. 

Let $P = \{p_1, p_2, \dots, p_N \}$ be the list of sets that will need to be recovered by header tags. The union of all these sets recovers the universe of all possible elements that we will see, $U =
\{1, 2, \dots, M\}$. Associated with
each element $j$ of the ground set $U$ is a weight term $w_j$. This weight term corresponds to the number of times the membership of $j$ is tested across the network. This will allow our algorithm to attempt to minimize the number of supersets that an element appears in if it needs to be tested for frequently. 

The goal of an algorithm which generates a superset encoding is to partition $P$ into $Z$ groups and union each group to create supersets $\{
s_1, s_2, \dots, s_Z \} = S$ such that the following function is minimized:

\begin{samepage}
$$ \min \sum_{j \in U} w_j \cdot a_j $$

Subject to

$$ \log_2{Z} + \max_{s_i \in S}\{s_i\} \le B $$

\end{samepage}

Where $a_j$ is the number of supersets that element $j \in U$ belongs to, and $B$ is the maximum number of bits that our tag can use. The objective function reflects the number of TCAM entries required for all membership tests. If the network tests for the membership of element $j$ $w_j$ times, and $j$ appears in $a_j$ supersets, then our approach requires $a_j w_j$ TCAM entries. 
The constraint shows that a superset construction is feasible if the identifier size plus the mask size is less than the bit constraint $B$. The identifier size is $\log_2{Z}$ if $Z$ is the number of supersets, and the mask size required is the maximum superset size, or $\max_{s_i \in S}\{s_i\}$.

We suspect that this problem is NP-Complete for \mbox{$M > B$}, but a proof of the claim is left as future work. 

\subsection{A Greedy Algorithm}

Although the problem may be hard, we can still formulate heuristics for constructing good-enough solutions. We begin with $N$ supersets, where superset $i$ is the $i$th list wish wish to recover. This solution is, by construction, able to generate every required element list, but it may not be feasible. $N$ may be so large that the superset identifier will dominate the metadata and exceed the bit limit. This solution will also certainly require a large number of TCAM entries, as no sets have yet been combined into supersets.

In the first step of the algorithm, we delete any supersets which are subsets of other supersets. If superset $s_i$ is $[A,B,C]$, and superset $s_j$ is $[B,C]$, then there is no use for $s_j$ and it may be deleted. Deletion strictly improves the solution because any subset of $s_j$ can still be recovered from $s_i$, and the superset identifier decreases in size as the number of supersets decreases. In practice, this reduces the number of sets down from hundreds of thousands to less than one hundred.

In step two, we attempt to greedily decrease the number of bits required by our feasible solution by merging pairs of small supersets together that do not increase the maximum mask size when unioned. The idea is that this will decrease the number of sets and thus the size of the superset identifier will decrease. This step repeats until every feasible merging action would increase the number of bits required. 

In step three, we improve the remaining supersets in an iterative greedy fashion. We consider all feasible mergings of pairs of supersets, where a feasible merge is a union of the two supersets which does not result in the new mask size exceeding the bit limit. The \textit{benefit} of a merging is the decrease in the number of flow rules which will result
from the merge. The decrease is the sum of the weights of each participant which appears in the intersection of the two supersets, where the weight is the number of flow rules in which the participant appears. This is because after a merging of two sets, every participant which appeared in the intersection now appears one less time across the supersets, and thus every rule they appear in can be replicated one less time. 

With these definitions in mind, the full algorithm is as follows:

\begin{algorithm}
 \KwData{feasible supersets $S = \{s_1, s_2, \dots, s_M\}$}
 \KwResult{a list of supersets with maximally decreased rule inflation }
remove subsets from $S$\;
let $A =$ the set of merge pairs which don't increase bits required\;
\While{A is nonempty}{
  choose pair $(s_i, s_j) = a \in A$ with smallest union\;
  merge sets $s_i$ and $s_j$\;
  update $A$\;
 }
let $A =$ the set of feasible merge pairs\;
remove any subsets from
\While{A is nonempty}{
  choose $(s_i, s_j) = a \in A$ with greatest benefit\;
  merge sets $s_i$ and $s_j$\;
  update $A$\;
 }
\end{algorithm}

The loops consider all pairs of supersets in each iteration in the worst case, so they have a quadratic running time. Fortunately, the removal of subset supersets in the first step, which runs linearly on lists of supersets with high redundancy, reduces the number of supersets in such lists to a small constant, so the running time is reasonable.

\subsection{Handling Updates}

We have given an algorithm for computing a static solution, but not all applications will be static. For middleboxes, new paths can be introduced, and existing paths can be modified. For IXP case, routes are announced and withdrawn continuously, meaning the list of valid next-hops for many prefixes are constantly changing. Therefore, we must give a procedure for handling dynamic updates to the superset matrix.

We begin with a feasible solution as output by our algorithm, which is a collection of supersets and a mask for each element set. We consider both cases of a new set or a modified set identically. We first generate a new superset for the new/modified set. If this new superset is a subset of an existing superset, we remove it. 

Each time a prefix is announced or withdrawn by a participant, we consider that prefix's new list of valid next-hops. If it is still a subset of some superset, we need only update the mask. 

If the new set is not a subset of any superset, we attempt to merge the new superset with an existing superset via the same greedy procedure we applied to the static context. If no merging is possible, we leave it as its own superset. If the introduction of the new superset causes the bit limit to be exceeded, we recompute the static solution entirely as a worst-case scenario. In our evaluations, we have found the worst-case scenario is never needed.


